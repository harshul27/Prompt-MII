import streamlit as st
import pandas as pd
from datasets import load_dataset
import random
from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt
from openai import OpenAI

# === Prompting Methods Implementation ===

def prompt_mii_instruction(subject, choices):
    """Prompt-MII: Meta-learned compact instruction"""
    return f"Task: {subject}. Choose from [{', '.join(choices)}]. Use contextual/logical clues."

def few_shot_prompt(subject, question, choices, shots=3):
    """Few-Shot: Multiple example demonstrations"""
    examples = []
    for i in range(shots):
        ex_q = f"Sample question {i+1} about {subject}"
        ex_ans = random.choice(choices)
        examples.append(f"Q: {ex_q}\nChoices: {', '.join(choices)}\nA: {ex_ans}")
    
    prompt = "\n\n".join(examples)
    prompt += f"\n\nQ: {question}\nChoices: {', '.join(choices)}\nA:"
    return prompt

def zero_shot_prompt(question, choices):
    """Zero-Shot: Minimal instruction"""
    return f"Question: {question}\nChoices: {', '.join(choices)}\nAnswer:"

def chain_of_thought_prompt(question, choices):
    """Chain-of-Thought: Step-by-step reasoning"""
    return f"Question: {question}\nChoices: {', '.join(choices)}\n\nLet's think step by step to find the correct answer:"

# === LLM API Inference (FIXED) ===

def llm_inference(prompt, choices, api_key, model="gpt-4o-mini"):
    """Real LLM inference via OpenAI API"""
    try:
        # Initialize client WITHOUT proxies parameter
        client = OpenAI(api_key=api_key)
        
        full_prompt = f"{prompt}\n\nRespond with ONLY the letter (A, B, C, or D):"
        
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a precise academic assistant. Answer with only the letter choice."},
                {"role": "user", "content": full_prompt}
            ],
            temperature=0.1,
            max_tokens=10
        )
        
        answer = response.choices[0].message.content.strip().upper()
        
        # Parse letter response
        if answer and answer[0] in 'ABCD':
            idx = ord(answer[0]) - 65
            if idx < len(choices):
                return idx
        
        # Fallback: match choice text
        for i, choice in enumerate(choices):
            if choice.lower() in answer.lower():
                return i
                
    except Exception as e:
        st.error(f"API Error: {str(e)[:200]}")
        return None
    
    return random.randint(0, len(choices)-1)

def count_tokens(text):
    return len(text.split())

def random_baseline(choices):
    return random.randint(0, len(choices)-1)

# === Streamlit App ===

st.set_page_config(layout="wide", page_title="Prompt-MII vs Traditional Prompting")

st.title("ðŸ”¬ Prompt-MII vs Traditional Prompting Techniques")
st.markdown("""
**Research Paper**: [Prompt-MII: Meta-Instruction Induction (arXiv:2510.16932)](https://arxiv.org/abs/2510.16932)

Compare **4 prompting techniques** with real LLM evaluation:
- **Prompt-MII**: Meta-learned compact instructions
- **Few-Shot**: Multiple example demonstrations  
- **Zero-Shot**: Minimal instruction baseline
- **Chain-of-Thought**: Step-by-step reasoning

ðŸ“Š Evaluate on MMLU benchmark with accuracy, F1, and token efficiency
""")

# Sidebar Configuration
st.sidebar.header("âš™ï¸ Configuration")

api_key = st.sidebar.text_input("OpenAI API Key", type="password", 
                                 help="Get key at: https://platform.openai.com/api-keys")

if not api_key:
    st.warning("âš ï¸ Please enter your OpenAI API key in the sidebar")
    st.info("ðŸ”‘ Get your API key at: https://platform.openai.com/api-keys")
    st.stop()

model = st.sidebar.selectbox("Model", ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"])
sample_size = st.sidebar.slider("Sample Size", 20, 100, 50, 10)
mmlu_subset = st.sidebar.selectbox("MMLU Domain", 
                                    ["business_ethics", "college_biology", "high_school_mathematics", 
                                     "professional_law", "us_foreign_policy"])
few_shot_k = st.sidebar.slider("Few-Shot Examples (K)", 2, 5, 3)

# Run Benchmark
if st.button("â–¶ï¸ Run Benchmark", type="primary"):
    
    # Load MMLU
    with st.spinner("Loading MMLU dataset..."):
        try:
            dataset = load_dataset("cais/mmlu", mmlu_subset, split="test")
            dataset = dataset.shuffle(seed=42).select(range(min(sample_size, len(dataset))))
            df = pd.DataFrame(dataset)
            
            df["choices"] = df["choices"].apply(lambda x: [str(c) for c in x])
            df["correct_answer"] = df.apply(
                lambda row: row["choices"][int(row["answer"])] 
                if str(row["answer"]).isdigit() and int(row["answer"]) < len(row["choices"])
                else "", axis=1
            )
            
            df = df[df["correct_answer"] != ""].reset_index(drop=True)
            
        except Exception as e:
            st.error(f"Dataset error: {e}")
            st.stop()
    
    st.success(f"âœ… Loaded {len(df)} samples from MMLU/{mmlu_subset}")
    
    # Initialize results
    results = {
        "Prompt-MII": {"predictions": [], "tokens": [], "correct": []},
        "Few-Shot": {"predictions": [], "tokens": [], "correct": []},
        "Zero-Shot": {"predictions": [], "tokens": [], "correct": []},
        "Chain-of-Thought": {"predictions": [], "tokens": [], "correct": []},
        "Random": {"predictions": [], "tokens": [], "correct": []}
    }
    
    progress = st.progress(0)
    status = st.empty()
    error_count = 0
    
    # Evaluate
    for idx, row in df.iterrows():
        progress.progress((idx + 1) / len(df))
        status.text(f"Evaluating {idx + 1}/{len(df)}...")
        
        subject = row["subject"]
        question = row["question"]
        choices = row["choices"]
        correct_idx = choices.index(row["correct_answer"])
        
        # Construct all prompts
        prompts = {
            "Prompt-MII": prompt_mii_instruction(subject, choices) + f"\n{question}",
            "Few-Shot": few_shot_prompt(subject, question, choices, few_shot_k),
            "Zero-Shot": zero_shot_prompt(question, choices),
            "Chain-of-Thought": chain_of_thought_prompt(question, choices)
        }
        
        # Get predictions from LLM
        for method, prompt in prompts.items():
            pred_idx = llm_inference(prompt, choices, api_key, model)
            
            if pred_idx is not None:
                results[method]["predictions"].append(pred_idx)
                results[method]["tokens"].append(count_tokens(prompt))
                results[method]["correct"].append(1 if pred_idx == correct_idx else 0)
            else:
                error_count += 1
        
        # Random baseline
        rand_pred = random_baseline(choices)
        results["Random"]["predictions"].append(rand_pred)
        results["Random"]["tokens"].append(0)
        results["Random"]["correct"].append(1 if rand_pred == correct_idx else 0)
    
    progress.empty()
    status.empty()
    
    if error_count > 0:
        st.warning(f"âš ï¸ Encountered {error_count} API errors")
    
    # Display Results
    st.markdown("## ðŸ“Š Benchmark Results")
    
    # Metrics table
    metrics_data = []
    for method, data in results.items():
        if data["predictions"]:
            acc = sum(data["correct"]) / len(data["correct"])
            avg_tokens = sum(data["tokens"]) / len(data["tokens"]) if data["tokens"] else 0
            metrics_data.append({
                "Method": method,
                "Accuracy": f"{acc:.1%}",
                "Avg Tokens": f"{avg_tokens:.0f}" if avg_tokens > 0 else "N/A",
                "Samples": len(data["predictions"])
            })
    
    metrics_df = pd.DataFrame(metrics_data)
    st.dataframe(metrics_df, use_container_width=True, hide_index=True)
    
    # Key insights
    if results["Prompt-MII"]["correct"] and results["Few-Shot"]["correct"]:
        col1, col2, col3 = st.columns(3)
        
        mii_acc = sum(results["Prompt-MII"]["correct"]) / len(results["Prompt-MII"]["correct"])
        mii_tokens = sum(results["Prompt-MII"]["tokens"]) / len(results["Prompt-MII"]["tokens"])
        
        fewshot_acc = sum(results["Few-Shot"]["correct"]) / len(results["Few-Shot"]["correct"])
        fewshot_tokens = sum(results["Few-Shot"]["tokens"]) / len(results["Few-Shot"]["tokens"])
        
        col1.metric("Prompt-MII Accuracy", f"{mii_acc:.1%}")
        col2.metric("Token Savings vs Few-Shot", f"{(1 - mii_tokens/fewshot_tokens)*100:.1f}%")
        col3.metric("Accuracy Difference", f"{(mii_acc - fewshot_acc)*100:+.1f}%")
        
        # Visualizations
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        # Accuracy
        methods = ["Prompt-MII", "Few-Shot", "Zero-Shot", "Chain-of-Thought", "Random"]
        accuracies = [sum(results[m]["correct"])/len(results[m]["correct"]) for m in methods if results[m]["correct"]]
        colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#808080']
        
        ax1.barh(methods[:len(accuracies)], accuracies, color=colors[:len(accuracies)])
        ax1.set_xlabel('Accuracy', fontweight='bold')
        ax1.set_title('Accuracy Comparison', fontweight='bold')
        ax1.set_xlim(0, 1)
        for i, v in enumerate(accuracies):
            ax1.text(v + 0.02, i, f'{v:.1%}', va='center', fontweight='bold')
        
        # Token usage
        token_methods = ["Prompt-MII", "Few-Shot", "Zero-Shot", "Chain-of-Thought"]
        avg_tokens = [sum(results[m]["tokens"])/len(results[m]["tokens"]) if results[m]["tokens"] else 0 
                      for m in token_methods]
        
        ax2.barh(token_methods, avg_tokens, color=colors[:4])
        ax2.set_xlabel('Avg Tokens per Query', fontweight='bold')
        ax2.set_title('Token Efficiency', fontweight='bold')
        for i, v in enumerate(avg_tokens):
            ax2.text(v + 5, i, f'{v:.0f}', va='center', fontweight='bold')
        
        plt.tight_layout()
        st.pyplot(fig)
        
        # Interpretation
        with st.expander("ðŸ“– Analysis & Insights"):
            st.markdown(f"""
### Key Findings

**Prompt-MII Performance:**
- Accuracy: **{mii_acc:.1%}**
- Avg Tokens: **{mii_tokens:.0f}**
- Token Savings: **{(1 - mii_tokens/fewshot_tokens)*100:.0f}%** vs Few-Shot

**Method Comparison:**
- **Prompt-MII**: Meta-learned compact instructions - optimal balance of efficiency and accuracy
- **Few-Shot**: Traditional approach with examples - highest token cost
- **Zero-Shot**: Minimal baseline - lowest tokens, variable accuracy
- **Chain-of-Thought**: Reasoning-focused - good for complex tasks
- **Random**: Sanity check baseline

**Business Impact**: Prompt-MII achieves competitive accuracy with {(1 - mii_tokens/fewshot_tokens)*100:.0f}% fewer tokens, directly reducing API costs.

**Model**: {model} | **Dataset**: MMLU/{mmlu_subset} | **Samples**: {len(df)}
            """)

st.markdown("---")
st.caption("Built by Harshul | Prompt Engineering Research | Based on arXiv:2510.16932")
